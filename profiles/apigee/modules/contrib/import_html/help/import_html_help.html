<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <!--
        /**
         * @file Import HTML Module for Drupal
         * @author Dan Morrison http://coders.co.nz/
         */
        -->
    <link type="text/css" rel="stylesheet" media="all"
    href="docs.css" />
<style type="text/css">
/*<![CDATA[*/
      .todo {color:00AA00;}
      .beta {color:#007700;}
/*]]>*/
</style>
    <title>
      Drupal Module: Import_HTML
    </title>
  </head>
  <body id="content">
    <h1>
      Drupal Module: Import_HTML
    </h1>
    <h2>
      Synopsis
    </h2>
    <p>
      Facility to import an existing, static HTML site structure
      into Drupal Nodes.
    </p>
    <p>
      This is done by allowing an admin to define a <a
      href="#source_siteroot">source directory (siteroot)</a> of a
      traditional HTML website, and importing (as much as possible)
      the content and structure into a Drupal site.
    </p>
    <p>
      Files will be absorbed completely, and their existing
      cross-links should be maintained, whilst the standard
      headers, chrome and navigation blocks should be stripped and
      replaced with Drupal equivalents. Old structure will be
      inferred and imported from the old folder hierarchy.
    </p>
    <ul>
      <li>
        <a href='#Requirements'>Requirements</a>
      </li>
      <li>
        <a href='#Usage'>Usage</a> Detailed step-by-step
      </li>
      <li>
        <a href='#Intent'>Intent</a> What it's intended to to 
        <ul>
          <li>
            <a href='#Methodology'>Methodology</a> Exactly how it
            does it, at a coder level
          </li>
          <li>
            <a href='#Notes'>Notes</a> Issues arising, and some
            detailed explanations
          </li>
        </ul>
      </li>
      <li>
        <a href='#Guide'>Guide</a> Reference section 
        <ul>
          <li>
            <a href='#Setup'>Setup</a> Requirements, and Installing
            for the first time
          </li>
          <li>
            <a href='#Templates'>Import Templates</a> XSL. With
            great power comes great complexity
          </li>
          <li>
            <a href='#Settings'>Settings</a> Explanation of the
            user settings
          </li>
        </ul>
      </li>
      <li>
        <a href='#Development'>Development / TODO</a> 
        <ul>
          <li>
            <a href='#Troubleshooting'>Troubleshooting</a> 
            <ul>
              <li>
                <a href="#open_basedir">open_basedir</a> (security)
              </li>
              <li>
                <a href="#max_allowed_packet">max_allowed_packet</a>
                (server death)
              </li>
              <li>
                <a href="#Relinking">Relinking</a>
              </li>
              <li>
                <a href="#Duplicate_content">Duplicate content</a>
              </li>
              <li>
                <a href="#pathauto">Pathauto conflicts</a> (menu not
                building right, and slow performance)
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h2 id="Requirements">
      Requirements
    </h2>
    <h3>
      Before you begin
    </h3>
    <p>
      See the <a href="#setup">setup section</a> for details.
      Because of the number of settings, this is not just a
      point-and-go module.
    </p>
    <ul>
      <li>
        <b>PHP5</b>. PHP4 support has been entirely dropped in 2010
      </li>
      <li>
        XML/XSLT support on the server. Check your php_info().
      </li>
      <li>
        HTMLTidy - Either with the PHP module or the commandline
        version.
      </li>
      <li>
        Some understanding of XSL for advanced template
        translation.
      </li>
      <li>
        Some libraries of my own (bundled) to actually do the XSLT
      </li>
    </ul>
    <h2 id="Usage">
      Usage
    </h2>
    <p>
      This module uses no database tables of its own. It requires
      XML support on the server, this can be tricky if it's not
      already enabled.
    </p>
    <p>
      Given a working system, the process is thus:
    </p>
    <ol>
      <li>
        Visit the <a
        href='&amp;base_url&amp;admin/build/import_html/profiles'>admin/build/import_html/settings</a>
        page and <a href='#Settings'>check the settings</a>.
        <br />
         Just use the 'default' import profile tab for now. <a
        href="#Profiles">Multiple profiles are advanced options</a>
      </li>
      <li>
        If all values look OK for now, you can try a test run by
        visiting admin/build/import_html/demo . Choose a 'page'
        sort of page, not a portal or layout-rich sort of thing.
        The demo will scrape the given file and import it to the
        system. Some of the new navigation features will not be
        apparent yet, as they apply only to large-scale imports, or
        at least imports that have a defined <a
        href="#source_siteroot">source siteroot</a>.
      </li>
      <li>
        Try opening the '<a
        href='&amp;base_url&amp;admin/build/import_html'>admin/build/import_html</a>'
        main page and defining a source folder. Enter the <a
        href="#source_siteroot">root path</a> of the site you wish
        to import and continue. <a href="#Treeview">The UI should
        display a treeview</a> of the files you can selectively
        select for import.
        <br />
         It's recommended to just try one page at a time to begin
        with.
        <br />
         <strong>Note:</strong> If your server has <a
        href="http://nz2.php.net/manual/en/features.safe-mode.php#ini.open-basedir">
        PHP open_basedir</a> restrictions in effect, the
        webserver/PHP process may be prevented from accessing files
        outside of webroot. <a href="#open_basedir">See below</a>
      </li>
      <li>
        Upon importing a page, a new node should be created. The
        object of the import templates is to trim down the
        <i>content</i> block to its unique value. This will
        <i>probably</i> require some template tuning, so make a new
        template (copy the existing html2simplehtml.xsl), select it
        (enter the new name in the admin page) <a
        href="http://www.dpawson.co.uk/xsl/sect2/sect21.html">tweak
        the XSL</a> and try again.
        <br />
         If you are extremely lucky, or don't care too much about
        the extras, you can go straight to bulk import.
      </li>
      <li>
        If you need to check how the the images are turning up,
        they can safely be imported as well using the previous
        interface. They will be copied, structured in the same
        folders they were in originally, into the directory
        configured in the admin/setting. Imported pages will have
        their <a href="#link_rewriting">links rewritten</a> to find
        them there.
        <br />
         Two type of content are being imported, depending on file
        suffix. 'Pages' (html) - which become nodes ... and
        everything else, which becomes 'files'.
      </li>
      <li>
        When you are happy that the body field is as tidy as it's
        going to get (test several pages), you can try a bulk
        import. This may fill up your node collection a bit, so be
        prepared to delete them if things don't work perfectly
        first time. Many static sites have whole sections that are
        not structured the same as the rest of the pages.
      </li>
      <li>
        On input, a menu structure and a bunch of aliases will be
        auto-generated. These can be manually adjusted easily. For
        instance, the menu branches will initially be named after
        the document titles found in the directory structure. Which
        is great if you used a decent folder heirachy, but some of
        the labels can probably be tidied up a bit. For that
        matter, after input, you can safely re-arrange the menu
        structure altogether, shifting whole sections to different
        places without worrying about links breaking. These changes
        will show through in the menu, sitemap and breadcrumbs
        <em>but not in the pathalias</em> which will reman
        old-style. <span class='beta'>There appear to be issues
        navigating to pages deep in a menu where the parent has not
        been imported or created yet. This is normal Drupal
        behaviour when making menu links to non-existant
        paths.</span>
      </li>
    </ol>
    <p>
      By following these instructions, you should probably be able
      to end up with a version of the old content in the new
      layout. For large sites (200+ pages) some extra tuning may be
      neccessary, eg using different templates for different
      sources.
    </p>
    <p>
      Incremental imports, processing just sections at a time, or
      repeated imports as you tune the content or the
      transformation should be non-destructive. Re-importing the
      same file will retain the same node ID path, and any
      Drupal-specific additions made so far.
    </p>
    <p>
      Multiple "Import Profiles" can be set up and saved alongside
      each other. This allows you to run side-by-side imports of
      different sources without over-writing the settings each
      time. You may find you require one profile for importing the
      'product information' part of a subsite, and another for
      importing the 'documentation archive' subdirectory. It is
      <em>mainly</em>em&gt; provided as a convenience to macro
      automation tools, and the normal user should just work with
      the 'default' profile and ignore the multi-profile feature.
    </p>
    <h2 id="Intent">
      Intent / Theory
    </h2>
    <p>
      This is intended as a run-once sort of tool, that, once tuned
      right on a handful of pages, can churn through a large number
      of reasonably structured, reasonably formatted pages doing a
      lot of the boring copy &amp; paste that would otherwise be
      required.
    </p>
    <p>
      The existing file paths of the source content will be used to
      create an automatic menu, and therefore a heirachical
      structure identical to the source URLs. With path.module,
      appropriate aliases will also be created such that this will
      enable a drupal instance to TRANSPARENTLY REPLACE an existing
      static site without breaking any bookmarks!
    </p>
    <h3 id='Methodology'>
      Methodology Overview / Tasks
    </h3>
    <p>
      A peek under the hood into what happens in what order
    </p>
    <ul>
      <li>
        We have a facility for spidering/enumerating existing
        source files. (the admin/build/import_html page)
      </li>
      <li>
        Define import rules - choose an XSL stylesheet, set some
        parameters on it, <span class='todo'>configure presets for
        the imported pages</span>.
      </li>
      <li>
        Expose selective selection of files to import (admin UI)
        <br />
         <img src='&amp;path&amp;import_html_select_files.png'
        alt='[screenshot]' />
      </li>
      <li>
        Import each source file by way of sequential : 
        <ol>
          <li>
            (Optional) <span class='beta'>download/copy of files to
            local mirror site.</span>
          </li>
          <li>
            Processing with html-tidy, to prepare for XSL
            transforms
          </li>
          <li>
            URL-rewriting via XSL. All hrefs are redirected to the
            new pseudo-location aliases, all srcs are redirected to
            somewhere under <kbd>/files</kbd>.
          </li>
          <li>
            Content-scraping via XSL (XSL stylesheet will probably
            have to be customized to each source site)
          </li>
          <li>
            Or content-scraping via RegExps and heuristics
          </li>
          <li>
            Deduction (as much as possible) of meta-information
            like page title, author,date
          </li>
          <li>
            <a href="#import_to_modules">Extra information can be
            added as hooks</a> specific to core or contrib modules.
          </li>
          <li>
            Validate nodes and save them with node-insert calls
          </li>
          <li>
            Extra API-insert calls (eg to create menu navigation
            and path aliases) are also called via module-specific
            hooks after save (required once we know what the new
            node ID is).
          </li>
        </ol>
      </li>
      <li>
        Pages are now first-class nodes, and can be administered
        through the CMS as usual.
      </li>
    </ul>
    <h3 id="Notes">
      Notes
    </h3>
    <p>
      The more valid and more homogenous the source site is, the
      better. A creation using strict XHTML and useful, semantic
      tags like <kbd>#title</kbd> <kbd>#content</kbd> or something
      could be imported swiftly. One with a variety of table
      structures may not...
      <br />
       Of course, this tool is supposed to be useful when dealing
      with messy, non-homogenous legacy sites that need a makeover.
      <span class='todo'>Sometimes regular expression parsing may
      come to the rescue for content extraction, but that's not
      implimented yet.</span>
    </p>
    <p>
      I'm choosing XSL because I know it, it's powerful for
      converting content out of (well-structured) HTML, and I've
      had success with this approach in the past. Others may object
      to this abstract technology (XSL is NOT an easy learning
      curve) but the alternative options include RegExp wierdness
      or cut and paste. (<span class='todo'>which I may patch on as
      alternative methods - or someone else can have a go</span>)
      Both approaches I've also used successfully in bulk site
      templating (over THOUSANDS of pages) but it's my call. <a
      href='#Templates'>Making your own XSL import template</a> is
      non-trivial.
    </p>
    <p>
      In the interests of good housekeeping, <b>imported files with
      spaces in the filenames will be renamed to use
      underscores</b>. Although it spaces <em>can</em> be worked
      around, they just cause trouble in website URLs. Thus,
      references to the spaced, or %20 versions of the files may
      break. This rewrite can be disabled in the settings.
      <br />
       Filenames are assumed to be, and will remain,
      case-sensitive.
    </p>
    <h2 id="Guide">
      Guide
    </h2>
    <h3 id="Setup">
      Installation/setup
    </h3>
    <h4>
      XML/XSL Support
    </h4>
    <p>
      The module uses the PHP5 implementation of XSL(T) but the PHP
      modules <b>does</b> have to be enabled somehow.
    </p>
    <p>
      If you can see the words XSL or XSLT in your phpinfo()
      output, You should be fine. The module will test and warn you
      anyway.
    </p>
    <p>
      If not,
    </p>
<pre>
    $ sudo apt-get install php5-xsl
</pre>
    <p>
      ... should do it for Debian/Ubuntu servers. Windows binary
      distributions I've seen come with it compiled in these days,
      but you may just have to uncomment a line
      <code>extension=php_xsl.dll</code>in php.ini to enable it.
    </p>
    <h4>
      HTMLTidy Setup
    </h4>
    <p>
      The module also uses <a
      href="http://www.w3.org/People/Raggett/tidy/">the famous
      HTMLTidy tool</a>. There is <a
      href="http://www.zend.com/php5/articles/php5-tidy.php">a PHP
      module that implements HTMLTidy natively</a>, which can be
      installed and enabled, either <a
      href="http://www.php.net/manual/en/tidy.installation.php">at
      PHP build-time</a>, or afterwards as a loaded extension.
    </p>
    <p>
      If you don't have (sudo) access to that, we can instead run
      'tidy' from the command line. Find the <a
      href="http://tidy.sourceforge.net/docs/Overview.html#download">
      appropriate binary release of HTMLTidy</a> for your system,
      and place it in your PATH, in the modules install directory,
      or wherever you like, then <em>define the path to the
      executable <a
      href='&amp;base_url&amp;admin/settings/import_html'>in the
      settings</a></em>. This works fine under Windows too.
    </p>
    <p>
      If this sounds complicated, and you have limited access to a
      Unix host and need to use it, there is an auto-installer (On
      the settings page under HTMLTidy configuration) that can
      attempt to set up tidy even on a box you don't have login
      access to.
    </p>
    <p>
      The preferred method is to enable the official, binary
      release tidy extension (not the PECL extension if you can
      help it). On some distros (Windows, Redhat) this is just a
      matter of uncommenting <code>extension=tidy.so</code> in your
      php.ini.
    </p>
    <h4>
      Ubuntu-PHP-tidy extension
    </h4>
    <p>

      <br />
      On Debian/Ubuntu, the quickest method to fetch and enable the
      extension is: <code>$ sudo apt-get install php5-tidy</code>
      If that works, you are good to go.
    </p>
    <p>
      In some systems, you may have to try compiling it for
      yourself. In Ubuntu (as of 2007) the tidy extension has been
      left out of the default debian PHP package :( <a
      href="http://packages.ubuntu.com/feisty/web/php5-tidy">although
      it may be found in certain repositories?</a>. <a
      href="http://www.coggeshall.org/oss/tidy/">Official
      instructions are to recompile php5 from source
      --with-tidy</a> but that's a bit scary if you are used to
      using a package manager.
      <br />
       Instead, <a
      href="http://ubuntuforums.org/showthread.php?t=195636">this
      post gives instructions on how to compile just the extension,
      then add it to php</a>. <small>I also had to <code>apt-get
      php5-dev</code> to get "phpize" on a brand new clean system,
      and had to use <code>./configure
      --with-tidy=tidy-20051018/</code> instead of just
      <code>./configure</code></small>
    </p>
    <h3 id="Templates">
      Import Templates
    </h3>
    <p>
      An import template defines the mapping between existing HTML
      content and our node values. It uses the XSL language because
      of the power it has to select bits of a structured document,
      for example <code>select=\"//*[@id='content']\"</code> ...
      will find the block anywhere in the page, of any type with
      the id 'content', and
      <code>select=\"//table[@class='main']//td[position(3)]\"</code>
      Will locate the third TD block in the table called 'main'.
      Both these examples would be common when trying to extract
      the actual <em>text</em> from a legacy site.
    </p>
    <p>
      You can begin with the example XSL template, this contains
      code that attempts to translate a page containing the usual
      HTML structures like (either title or h1) and (either the div
      called 'content' or the entire body tag) into a standard,
      minimal, vanilla, sematically-tagged HTML doc.
    </p>
    <p>
      It's likely that whatever site you are importing will NOT be
      shaped exactly like we need it to translate straight using
      this format. You have to identify the parts of your existing
      pages that can reliably be scanned for to define content,
      then come up with an <a
      href='http://www.w3.org/TR/xpath'>XPath expression</a> to
      represent this.
    </p>
    <p>
      If your source, for example, didn't use nice H1 tage to
      denote the page title, but instead always looked like
    </p>
<pre>
&lt;font size='+2'&gt;&lt;B&gt;my
  page&lt;/B&gt;&lt;/font&gt;
</pre>
    <p>
      ... your template could be made to find it, wherever it was
      in the page using
      <code>select=\"//font[@size='+2']/B\"</code> and proceed to
      use that as the node title.
    </p>
    <p>
      No, the code is not pretty, and if Regular Expressions are a
      foreign language to you, This is worse.
      <br />
       But <em>this</em> is why developers have been ranting for
      the last ten years about using semantic markup!!
      <br />
       The uniformity, and the usefulness of the metadata detected
      in the source files will play a big part here.
    </p>
    <p>
      It's easier to develop and test the XSLT using a third-party
      tool, I recommend <a
      href="http://www.xmlcooktop.com/">Cooktop</a> on Windows, or
      <a href="http://www.oxygenxml.com/">oXygen</a> on everything
      else. Be sure to set the XSL engine to 'Sablotron' which is
      the one that PHP uses under the hood.
    </p>
    <p>
      <span class='todo'>Although it would be <em>possible</em> to
      configure a logical mapping system to select different import
      templates based on different content, at this stage the
      administrator is expected to be doing a bit of hand-tweaking,
      and predicting all possible inputs is impossible.</span>
      <span class='beta'>Some of this sort of logic <em>can</em>
      however be built into the powerful XSL template, if you are
      good at XSL</span>
    </p>
    <p>
      Once importing is taking place, you can even filter it more
      to improve the structure of the input, for example by
      removing all redundant FONT tags, or by ensuring that every
      H1,2,3 tag has an associated #ID for anchoring. Yay XSL.
    </p>
    <h4>
      Your own templates
    </h4>
    <p>
      To start with, you can use the
      <code>html2simplehtml.xsl</code> template. That contains some
      logic that makes generic guesses about <em>any</em> source
      structure. <b>You are best to NOT use this as a base</b> for
      modification if developing your own template, as the extra
      logic there may be unwanted. For a starter template, use the
      much simpler <code>simplehtml2simplehtml.xsl</code> sample
      instead.
    </p>
    <h4 id='import_to_taxonomy'>
      Import to Taxonomy
    </h4>
    <p>
      If you have taxonomy enabled and the source is tagged, these
      terms can be imported. Links with a rel='tag' attribute will
      be taken to refer to keywords, tags or terms in your
      available taxonomy. Each of the following syntaxes should be
      equivalently detected as the term 'Interesting':
    </p>
<pre>
  &lt;a href='whatever/term' rel='tag' &gt;interesting&lt;/a&gt;
  &lt;link href='whatever/term' rel='tag' title='interesting' /&gt;
  &lt;meta name="keywords" content="interesting" /&gt; 
</pre>
    <p>
      First - if an existing term of that name exists in any valid
      vocabulary, the imported page will be tagged with it. If not,
      the <em>first</em> available freetagging vocabulary will be
      used to insert the tag. If no freetagging is enabled, only
      pre-existing terms will be used.
    </p>
    <h4 id='import_to_taxonomy'>
      Import to Chosen Taxonomy
    </h4>
    <p>
      Although very rarely used, it's possible to specify the
      target vocab with syntax such as:
    </p>
<pre>
  &lt;a href='whatever/term' rel='tag' &gt;subject:interesting&lt;/a&gt;
  &lt;a href='places/77' rel='tag' &gt;location:aotearoa&lt;/a&gt;
</pre>
    Which will place the page as 'Interesting' in the 'Subject'
    vocabulary and 'Aotearoa' in the 'Location' vocabulary. Raw
    imports probably won't have this level of namespace, but you
    can use it to translate contextual information in the page into
    import clues using the XSL template. EG, this can be translated
    quite easily in XSL: 
<pre>
  &lt;div class='subjects'&gt;
    &lt;h3&gt;More:&lt;h3&gt;
    &lt;a href='whatever/term/13' rel='tag' &gt;Interesting&lt;/a&gt;
    &lt;a href='whatever/term/funny' rel='tag' &gt;Funny&lt;/a&gt;
  &lt;/div&gt;
  &lt;div class='location'&gt;
    &lt;h3&gt;Places:&lt;h3&gt;
    &lt;a href='places/aotearoa' rel='tag' &gt;Aotearoa&lt;/a&gt;
  &lt;/div&gt;
</pre>
    <h4 id='import_to_cck'>
      Import to CCK
    </h4>
    <p>
      The base functionality supports placing found content into
      the <code>$node-&gt;body</code> field, not naturally into any
      arbitrary CCK fields, but this is also possible.
    </p>
    <p>
      If you have a CCK node with (eg) fields:
    </p>
<pre>
field_text, field_byline, field_image
</pre>
    and your input pages are nice and semantically tagged, eg 
<pre>
&lt;body&gt;
  &lt;h1 id='title'&gt;the title&lt;/h1&gt;
  &lt;div id='image'&gt;&lt;img src='this.gif'/&gt;&lt;/div&gt;
  &lt;h3 id='byline'&gt;By me&lt;/h3&gt;
  &lt;div id='text'&gt;the content html etc&lt;/div&gt;
&lt;/body&gt;
</pre>
    <p>
      A mapping from HTML ids to CCK fields will be done
      automatically, and the content <em>should</em> just fall into
      place.
    </p>
<pre>
  $node-&gt;title = "the title";
  $node-&gt;field_image = "&lt;img src='this.gif'/&gt;&lt;";
  $node-&gt;field_byline = "By me";
  $node-&gt;field_text = "the content html etc";
</pre>
    <p>
      (Actually, current CCK field notation internally places all
      field values into an array with a 'value'. This is correctly
      supported during import via the
      <code>modules/content.inc:content_import_html()</code> hook.
      .)
    </p>
    <h4>
      Import sidebar/pullquote
    </h4>
    <p>
      It's common that imported source may contain related non-body
      content you want to capture.
    </p>
    <p>
      First - edit your target content type to include a multiline,
      multivalue text field called 'field_sidebar'
    </p>
    <p>
      Any source data with the class or id 'sidebar' will now
      arrive into that textarea. The XSL may need to be adjusted,
      eg like so:
    </p>
<pre>
  &lt;xsl:template name="sidebar" match="//*[id='leftcol']"&gt;  
    &lt;div id="sidebar"&gt;
      &lt;xsl:apply-templates /&gt;
    &lt;/div&gt;
  &lt;/xsl:template&gt;
</pre>
    <p>
      The above snippet will find any 'leftcol' content and store
      it into your own 'sidebar' field. This field can then be
      rendered as you wish within Drupal, eg by using
      cck_block.module to put it BACK into a column within your
      theme :-)
      <br />
       This method can be extended a lot.
    </p>
    <p>
      In fact, ANY element found in the source text with an ID or
      class gets added to the <code>$node</code> object during
      import, although most data found this way is immediately
      discarded again if the content type doesn't know how to
      serialize it. Enabling debugging will display the full node
      object as it exists before saving. Inspecting that data
      structure may help you tune a storage space in your target
      content type.
      <br />
       A special-case demonstrated here prepends
      <code>field_</code> to known CCK field names. Normally they
      get labelled as-is.
    </p>
    <p>
      If the source data is NOT tagged, you'll have to develop a
      bit of custom XSL to produce the same effect.
    </p>
    <h4>
      customtemplate2simplehtml.xsl
    </h4>
<pre>
... xsl preamble ...
  &lt;xsl:template name="html_doc" match="/"&gt;  
    &lt;html&gt;
    &lt;body&gt;
    ... other extractions ...
    &lt;h3 id="byline"&gt;
      &lt;xsl:value-of select="./descendant::xhtml:img[2]/@alt" /&gt;
    &lt;/h3&gt;
    &lt;/body&gt;
  &lt;/html&gt;
&lt;/xsl:template&gt;
</pre>
    <p>
      In this example, the byline we wanted to extract was the alt
      value of the second image found in the page (a real-world
      example). This has now been extracted and wrapped in an ID-ed
      h3 during an early phase of the import process, and should
      now turn up in the CCK field_byline as desired.
      <br />
       XSL is complex, but magic.
    </p>
    <h4 id='import_to_modules'>
      Import to Other Modules
    </h4>
    <p>
      Any add-on modules can use a hook to extract their own data
      from the input file and add data to the node object.
      <br />
       The <code>modules/</code> directory contains the callbacks
      that allow any module access to the half-cooked node object
      and raw source data to extract and insert/modify its own node
      properties before it is saved.
      <br />
       Contrib examples are in the <code>import_html/modules</code>
      directory. See that for the hook prototype and docs.
    </p>
    <p>
      If, for example you were able to extract date/time
      information out of an import page, event.module could be told
      to do so, and create detailed event nodes.
    </p>
    <h4>
      Other XML!
    </h4>
    <p>
      I've sucessfully used it to import other random XML formats
      (RecipeML) although the advantages of doing so are currently
      limited.
    </p>
    <p>
      If it is possible for you to create an XSL template that
      translates any arbitrary XML dialect into the 'simple' HTML +
      microformat markup used during the import phase (see
      examples) then your XML can be imported into Drupal nodes.
      <b>It is supported for one source file to produce multiple
      Nodes</b>. The 'simple' HTML half-way phase should be an XML
      document containing multiple HTML elements. Wrapping them in
      xt:document nodes is a good idea. (Example needs to be given)
    </p>
    <h3 id="Settings">
      Settings
    </h3>
    <p>
      On the <a
      href='&amp;base_url&amp;admin/build/import_html/settings'>Administer
      - Site building - Import HTML</a> screen, you can (if you
      wish):
    </p>
    <ul>
      <li>
        Choose the <b><a href='#Templates'>import template</a></b>.
        These templates translate between the existing page
        structure and the raw content blocks. XSL templates are
        supplied in the modules '<code>templates/</code>'
        directory. Place your own templates there.
        <br />
         Use the existing examples to start from.
        <code>simplehtml2simplehtml.xsl</code> is the easiest to
        build on. <code>html2simplehtml.xsl</code> attempts to be
        generic, so includes a bunch of catch-all logic.
      </li>
      <li>
        Customize a parameter used in the import template - the
        <b>id of the real 'content' block</b> of the source
        documents. <span class='todo'>This could be extended into a
        wizard to work towards an all-purpose template, but that
        will probably never happen. Can't predict how broken the
        import sites may be.</span>
      </li>
      <li>
        When a site is imported, it must bring along some of its
        baggage. Images and suchlike. You can choose where they
        will end up as the <b>Extra File Storage Path</b>.
      </li>
      <li>
        When the imported site is given new URLs (reflecting the
        original path) we can publish the new nodes in a 'subsite'
        by applying a <b>prefix</b> directory to the aliases they
        are issued. The existing old links will be written to point
        to where the imported neighbouring pages are EXPECTED to
        end up. Incremental processing means they may not always be
        there until the whole site is done. Link checking
        (preferably on-the-fly) would be a nice tidy-up process.
      </li>
      <li>
        The new URLs generated for the new pages are <b>url
        aliases</b> based on the original paths. You can choose to
        have tidy (no suffix) or legacy (old .htm or whatever
        suffix) aliases - or both. path.module is required for
        this.
      </li>
      <li>
        As the input is (a fragment of) Pure HTML, the content
        filter (<b>input format</b>) must be set correctly. I
        choose to define a blank filter, which doesn't even add
        extra BRs, but you can override that if you wish.
      </li>
      <li>
        If using non-native HTMLTidy support, the path to the tidy
        executable should be defined <a
        href="&amp;base_url&amp;admin/build/import_html/setup">in
        the setup</a>. PHP safe mode or settings can cripple this,
        so you may be out of luck on cheap hosting servers.
      </li>
    </ul>
    <h4 id="Treeview">
      Notes on the Treeview Interface
    </h4>
    <p>
      Files and folders beginning with _ or . are nominally
      'hidden' so are skipped and do not show up on this listing.
      While it's possible to list a thousand or so files, It may be
      a good idea to allow the listing to be more selective, to
      scale to larger sites. Do this by entering the
      <strong>Subsection to list</strong> <em>before</em> clicking
      list and waiting for every file on the server to be
      enumerated.
    </p>
    <h2 id="Development">
      Development / TODO
    </h2>
    <p>
      As mentioned in Usage, this module uses no database tables of
      its own. Pages are read straight into 'page' nodes.
    </p>
    <p>
      It's easy to imagine this sytem set up as a synchroniser,
      that could re-fetch and refresh local nodes when remote
      content changes. This would involve recording exactly what
      the source URL was (which isn't currently done) but would be
      a fun feature.
    </p>
    <p>
      I may fork off the page-parsing into a pluggable method, so
      that a regexp version can be developed alongside, and be used
      for folk without XSL support.
    </p>
    <p>
      How to leverage this to import a local site to a remote
      server? You must either unpack the source files somewhere on
      that machine, then provide the absolute path where the server
      can find them.
    </p>
    <p class='todo'>
      Also TODO is a 'Spidering' method to try to import URL sites.
      Way in the future!
    </p>
    <p class='todo'>
      TODO There are issues when a page links directly to a file
      that would be regarded as a resource via an href. Most hrefs
      are re-written to point to the new node, but things like
      large images or word docs get imported under 'files'. The XSL
      rewrite_href_and_src.xsl attempts to correct for this, but
      there may be some side-effects. Always run a link checker
      after import.
    </p>
    <h3 id='Troubleshooting'>
      Troubleshooting
    </h3>
    <h4 id="open_basedir">
      open_basedir
    </h4>
    <p>
      If your server has <a
      href="http://nz2.php.net/manual/en/features.safe-mode.php#ini.open-basedir">
      PHP open_basedir</a> restrictions in effect, the
      webserver/PHP process may be prevented from accessing files
      outside of webroot. This is a good security measure, but may
      stop import_html from reading your source data (even though
      browsing the source directories may still appear to work).
      The <code>open_basedir</code> setting can be seen in your
      phpinfo.
      <br />
       An error like: <code>Local file copy failed
      (/tmp/1fixed/simple.htm to files/imported/simple.htm)</code>
      When you are sure the source file <strong>does</strong> exist
      and permissions <strong>are</strong> readable may be
      symptomatic of good security on your server. A reasonable fix
      is to place your source data inside webroot/files (even if
      just temporarily) to run the import process, then delete it
      later. Alternatively, copy your data over top of web root (as
      described in walkthrough.htm) to do an in-place import.
      Disabling open_basedir is not recommended, and probably
      requires root privileges anyway. <a
      href="http://drupal.org/node/103221">Drupal.org issue
      discussion</a>
    </p>
    <h4 id="max_allowed_packet">
      max_allowed_packet
    </h4>
    <p>
      It has been found that there is
      a limit to how many batch operations you can queue up at one
      time. If you get a white screen, and error in your log saying
      something like: <code>Got a packet bigger than
      'max_allowed_packet' bytes</code> It means the server is
      trying to do just too damn much. The <em>list of
      instructions</em> is too long to fit into one database entry!
      The <a
      href="http://dev.mysql.com/doc/refman/5.1/en/packet-too-large.html">
      max_allowed_packet limit can be increased in your MySQL
      configuration</a> or <em>possibly</em> from code. import_html
      includes an attempt to fix this problem automatically - but
      MAY NOT ALWAYS WORK on some hosts.
      <br />
       If your host has such a limit, you will have to take imports
      slowly, and only select and process a few hundred pages at a
      time.
    </p>
    <h4 id="Relinking">
      Relinking
    </h4>
    <p>
      I've gone to great lengths to rewrite the links from the new
      node locations to <em>relative</em> links to the resources
      that moved over into /files/ but there are problems. When
      <code>a/long/path/index.html</code> links to its image by
      going <code>../../../files/a/long/path/pic.jpg</code> it
      <b>works</b> which is good. But as
      <code>a/long/path/index.html</code> is also aliased to
      <code>a/long/path</code> - that up-and-over path is
      <em>wrong</em> now the page is being served from what looks
      to the browser like a different place.
    </p>
    <p>
      I don't favour embedding anything that hard-codes the Drupal
      base_url, and we don't want to use HTML BASE. I want to
      continue to support portable subsites, so embedding
      site-rooted links (<code>/files/etc</code>) is not great
      either.
    </p>
    <p>
      Currently, by happy chance, going up one <code>../</code> too
      far will get <em>ignored</em> by most browsers, so if you are
      <em>not</em> running Drupal in a subdirectory, the requests
      for both style of page <em>will</em> just work. Which will
      mean that 80% of cases should get by OK. The rest may need an
      output filter of some sort developed some day
    </p>
    <h4 id="Duplicate_content">
      Duplicate content
    </h4>
    <p>
      If you find that duplicate, identical menu items are created
      (both '/mycontent' and '/mycontent/mycontent') or that child
      items are created under inaccessable or non-existant parents,
      check the 'Default Document' in the settings. The process
      will interpret 'index.htm' and 'index.html' differently, and
      only the correct one can be used as the parent item. Enter
      the filename appropriate to the site you are importing from.
      <br />
       If you are using both interchangably on one site .. :-{
    </p>
    <h4 id="pathauto">
      Pathauto
    </h4>
    <p>
      import_html attempts to assign correct paths to all imported
      content. The menu building process actually requires that
      improted files be given new paths that match exactly where
      the content was in the original site. <em>Pathauto can
      conflict with that</em> and ruin things.
      <br />
       If you are using pathauto, it should be disabled during
      imports, or it will come up with its own paths and rename
      nodes as we import content, meaning the importer will
      immediately lose track of new nodes, and the menu builder
      will fail. It may be possible to set pathauto to only add to
      (and not remove) existing aliases. This means that the import
      process will work OK, but your rules will mean that all
      imported content will have unneccessary and probably
      inaccurate path aliases added to it.
      <br />
       Also, pathauto has been profiled to be <em>incredibly</em>
      inefficient when operating over a large number of items, so
      it's best to turn it off when using import_html. You can turn
      it on again later.
    </p>
    <h3 id="glossary">
      Glossary / Terminology
    </h3>
    <dl>
      <dt id="source_siteroot">
        Source Siteroot
      </dt>
      <dd>
        The root of the input site. This may be a folder on the
        local filesystem, possibly beginning with "/". Imported
        files will calculate their links relative to there. If the
        source files have all their links <em>truly relative</em>
        (no links starting with '/' or 'http') then links will be
        rewritten as normal. If a link is found that starts with
        '/' or 'http://site.name/' (<b>root-relative</b>) then this
        will be converted to behave as if it were starting from the
        <em>Source Siteroot</em>.
        <br />
         EG: <code>/var/www/old_site/html/</code>
      </dd>
      <dt id="source_subsection">
        Source Subsection
      </dt>
      <dd>
        A folder <em>within Source Siteroot</em> to process. Links
        will still be recalculated relative to the Source Siteroot,
        but only the subsection will be displayed for selection.
        Use this to manage large numbers of files that may take a
        while to prepare. EG: <code>archives/2007</code>
      </dd>
      <dt id="link_rewriting">
        Link Rewriting
      </dt>
      <dd>
        Links will be re-written according to the rules in the
        settings. <em>No actual link-checking is done</em>, the
        links are rewritten to the location the other files <em>are
        expected to be</em> so if you are processing subsections or
        selections from the tree, they may point to places that
        don't exist yet.
        <br />
         This means that imports are 'atomic' and it is safe to
        import or re-import pages individually without knowing what
        has happened before or will happen.
        <br />
         You are advised to run a link-checker afterwards. 
        <p>
          There is provision for <strong>pages</strong> and
          <strong>resources</strong> to be placed in different
          places in the site. In general, pages will be accessed
          under the normal site root, while Drupal conventions
          place images and documents into a /sites/site.name/files
          folder. Most legacy sites have images in /images etc.
          This needs to become
          <code>/sites/site.name/files/images</code> within Drupal.
          The rewriting does this, by detecting the <em>suffix</em>
          of the file being linked to in href= or src= tags. Any
          file type that is <em>not</em> in the list of known HTML
          file types is regarded as a 'resource' and rewritten to
          appear under the 'files' directory. <small>The
          $import_html_file_classes array is currently hard-coded
          in the module.</small> <span class='beta'>File suffixes
          are not good enough for this, should the suffix list be
          editable, or should I scan the files themselves?</span>
        </p>
        <p class='todo'>
          TODO Hard http://old.site/ links-to-self may need to be
          removed using a different process.
        </p>
      </dd>
    </dl>
    <h4 id="Profiles">
      Alternative Profiles
    </h4>
    <p>
      Most users will not have a use for this in the space of one
      project, but it is possible to save several different import
      profiles (sets of settings) through the UI, and switch
      between them as you go.
    </p>
    <p>
      This feature is intended to be used by other automation
      processes, or for Large projects where you want to import all
      the 'gallery' pages with one set of rules, and the 'product'
      pages with another set.
    </p>
    <p>
      Import HTML profiles are exportable nad importable using the
      'Features' module, so they provide a way to bundle your
      settings.
    </p>
    <p>
      It's probably not worth playing with if you just want one
      site imported.
    </p>
    <h3 id="reference">
      Reference
    </h3>
    <p>
      Long ago, I started building this with reference to the
      existing <a href='http://drupal.org/node/14858'>import/export
      module</a> but I couldn't find too many common features. The
      transitional format the XSL templates convert into is a
      'microformat' of XHTML (basically XHTML, but with strictly
      controlled classes and IDs). This is how I see a
      platform-agnostic dump of content should be exported, when
      this eventually morphs into import_export_HTML.
    </p>
  </body>
</html>

